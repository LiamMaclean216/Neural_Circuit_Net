{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from masked_cross_entropy import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sequences(length_from, length_to,\n",
    "                     vocab_lower, vocab_upper,\n",
    "                     batch_size):\n",
    "    \"\"\" Generates batches of random integer sequences,\n",
    "        sequence length in [length_from, length_to],\n",
    "        vocabulary in [vocab_lower, vocab_upper]\n",
    "    \"\"\"\n",
    "    if length_from > length_to:\n",
    "            raise ValueError('length_from > length_to')\n",
    "\n",
    "    def random_length():\n",
    "        if length_from == length_to:\n",
    "            return length_from\n",
    "        return np.random.randint(length_from, length_to + 1)\n",
    "    \n",
    "    while True:\n",
    " \n",
    "        \n",
    "        padded = np.zeros([batch_size,length_to])\n",
    "        seq_lengths = np.zeros([batch_size])\n",
    "        for i in range(batch_size):\n",
    "            rand = np.random.randint(low=vocab_lower,\n",
    "                              high=vocab_upper,\n",
    "                              size=random_length()).tolist()\n",
    "            seq_lengths[i] = len(rand)\n",
    "            padded[i,0:len(rand)] = rand\n",
    "            \n",
    "       \n",
    "        concat = np.zeros([batch_size,padded.shape[1]+1])\n",
    "        concat[:,0] = seq_lengths\n",
    "        concat[:,1:concat.shape[1]] = padded\n",
    "        \n",
    "        yield concat[:,1:],concat[:,0].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=3, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        self.hidden = None\n",
    "    def forward(self, input_seqs, input_lengths):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        #embedded = self.embedding(input_seqs)\n",
    "        input_seqs = input_seqs.unsqueeze(-1).type(torch.FloatTensor)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seqs, input_lengths)\n",
    "        outputs, self.hidden = self.gru(packed, self.hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,output_size,hidden_size,enc_hidden_size,n_layers=3):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(enc_hidden_size, hidden_size, n_layers, bidirectional=True)\n",
    "        self.hidden = None\n",
    "       \n",
    "        self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, encoder_outputs):\n",
    "        rnn_in = encoder_outputs[-1].unsqueeze(0)\n",
    "           \n",
    "        outputs, self.hidden = self.gru(rnn_in, self.hidden)\n",
    "        outputs = self.concat(outputs)\n",
    "        out = self.out(outputs)\n",
    "\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 4.6999797098835305  Real : [2. 3. 7.]  Pred : [0.00351813 0.02101257 0.03675535]\n",
      "Loss : 2.173477840423584  Real : [1. 8. 2.]  Pred : [4.1021719  5.03765106 5.20327425]\n",
      "Loss : 1.7978454717000325  Real : [1. 6. 2.]  Pred : [4.0892868  4.73458338 4.83386135]\n",
      "Loss : 1.918986333211263  Real : [6. 2. 3.]  Pred : [3.99772978 4.51270819 4.5884552 ]\n",
      "Loss : 1.4174486176172891  Real : [5. 3. 8.]  Pred : [4.95175219 5.45887089 5.27206278]\n",
      "Loss : 0.9013707717259725  Real : [8. 2. 4.]  Pred : [7.41629553 5.73517466 3.810179  ]\n",
      "Loss : 0.9992927519480387  Real : [4. 6. 2.]  Pred : [4.12141323 3.6221261  2.34722829]\n",
      "Loss : 0.8726032161712647  Real : [5. 4. 7.]  Pred : [5.09262323 6.56126022 7.09937906]\n",
      "Loss : 0.8284240237871806  Real : [1. 8. 8.]  Pred : [1.76905215 5.91340923 8.15221214]\n",
      "Loss : 0.339986310005188  Real : [6. 6. 7.]  Pred : [5.68950462 5.82704687 6.72315502]\n",
      "Loss : 0.1674202076594035  Real : [1. 7. 2.]  Pred : [1.15956974 6.4580965  2.22168398]\n",
      "Loss : 0.13267957290013632  Real : [1. 4. 1.]  Pred : [0.96864599 4.05677652 1.22598481]\n",
      "Loss : 0.09982735315958659  Real : [8. 3. 4.]  Pred : [7.88288116 2.92685485 4.04135942]\n",
      "Loss : 0.12393319765726725  Real : [6. 1. 6.]  Pred : [5.90131426 0.90866059 5.88448381]\n",
      "Loss : 0.079208025932312  Real : [2. 5. 1.]  Pred : [2.01341391 4.94612789 1.06780219]\n",
      "Loss : 0.10618758201599121  Real : [1. 6. 8.]  Pred : [1.18874967 6.14117002 7.98956251]\n",
      "Loss : 0.06429446458816529  Real : [7. 3. 8.]  Pred : [7.03582001 3.04955006 7.9089179 ]\n",
      "Loss : 0.05498315095901489  Real : [4. 4. 3.]  Pred : [4.10072231 4.03282118 3.01996398]\n",
      "Loss : 0.08237559795379638  Real : [8. 2. 2.]  Pred : [8.03781319 1.92348015 1.98922694]\n",
      "Loss : 0.08866286118825277  Real : [1. 7. 4.]  Pred : [0.93945122 7.13128376 4.0070982 ]\n",
      "Loss : 0.04821276028951009  Real : [6. 5. 4.]  Pred : [5.97489119 4.96532106 3.91777658]\n",
      "Loss : 0.05392405033111572  Real : [2. 8. 5.]  Pred : [1.97817171 8.07805634 5.08319998]\n",
      "Loss : 0.04212593634923299  Real : [1. 3. 8.]  Pred : [1.06765497 3.10571122 8.00701427]\n",
      "Loss : 0.08949812491734822  Real : [8. 3. 6.]  Pred : [8.11526775 3.03977823 6.09680462]\n",
      "Loss : 0.047080613772074384  Real : [7. 8. 4.]  Pred : [7.10592842 8.01588631 4.06613159]\n",
      "Loss : 0.05086927890777588  Real : [1. 1. 7.]  Pred : [0.99038082 0.96925038 7.04021311]\n",
      "Loss : 0.04057629028956095  Real : [7. 5. 6.]  Pred : [7.02554083 4.89633656 5.99417067]\n",
      "Loss : 0.09092265129089355  Real : [4. 7. 7.]  Pred : [3.98224306 7.15893173 7.08323574]\n",
      "Loss : 0.025036367575327557  Real : [1. 3. 8.]  Pred : [1.07808697 3.0313189  7.97212315]\n",
      "Loss : 0.057674131393432616  Real : [3. 6. 6.]  Pred : [3.05062079 5.9218483  6.01855803]\n",
      "Loss : 0.08382656494776408  Real : [3. 2. 3.]  Pred : [2.99183106 2.10880303 3.11633039]\n",
      "Loss : 0.029898234208424888  Real : [4. 5. 4.]  Pred : [4.00630093 4.94303083 3.94797468]\n",
      "Loss : 0.03174009879430135  Real : [6. 1. 8.]  Pred : [6.05240774 1.03437948 7.91592741]\n",
      "Loss : 0.050732853412628176  Real : [8. 4. 2.]  Pred : [7.99805069 4.0512538  2.08842897]\n",
      "Loss : 0.08067811171213786  Real : [3. 2. 7.]  Pred : [3.02002716 2.07349682 7.10649204]\n",
      "Loss : 0.027599684397379556  Real : [2. 5. 2.]  Pred : [2.00419688 5.06288815 2.02497864]\n",
      "Loss : 0.03310495058695475  Real : [4. 6. 5.]  Pred : [3.99515367 6.02776718 4.96729898]\n",
      "Loss : 0.029824754397074382  Real : [6. 7. 3.]  Pred : [5.99741077 6.93877602 3.03187251]\n",
      "Loss : 0.0298076327641805  Real : [4. 1. 8.]  Pred : [4.02833557 1.01670158 7.94332457]\n",
      "Loss : 0.03738521099090576  Real : [8. 6. 1.]  Pred : [7.9969182  6.03419161 1.02035713]\n",
      "Loss : 0.049255967140197754  Real : [4. 1. 2.]  Pred : [4.01304865 0.92464739 2.05526638]\n",
      "Loss : 0.028072166442871093  Real : [1. 1. 2.]  Pred : [1.01215088 1.04079103 1.92332768]\n",
      "Loss : 0.05138045310974121  Real : [7. 7. 6.]  Pred : [6.9347105  7.00833416 6.07162046]\n",
      "Loss : 0.032150390148162844  Real : [4. 5. 6.]  Pred : [4.00607252 5.00345421 5.99911118]\n",
      "Loss : 0.034456509749094644  Real : [3. 4. 7.]  Pred : [3.01340771 4.00613356 7.01500082]\n",
      "Loss : 0.02096595605214437  Real : [5. 4. 1.]  Pred : [4.99483967 4.00184155 0.96450031]\n",
      "Loss : 0.03311796426773071  Real : [6. 7. 2.]  Pred : [6.02084398 7.0133729  2.00901198]\n",
      "Loss : 0.07531865914662679  Real : [3. 6. 1.]  Pred : [3.03937864 5.8464222  1.02560389]\n",
      "Loss : 0.042033300399780274  Real : [3. 3. 3.]  Pred : [2.97916245 3.01660132 3.07609844]\n",
      "Loss : 0.038304255803426106  Real : [8. 2. 7.]  Pred : [8.02184582 1.9284606  7.08531618]\n",
      "Loss : 0.043812909921010335  Real : [5. 4. 2.]  Pred : [4.97969913 4.1084528  2.01279354]\n",
      "Loss : 0.04187293450037639  Real : [4. 7. 6.]  Pred : [4.03884315 7.0268259  5.91953182]\n",
      "Loss : 0.03143508434295654  Real : [2. 2. 3.]  Pred : [2.01853228 2.04824066 3.04996514]\n",
      "Loss : 0.03918528000513712  Real : [8. 5. 6.]  Pred : [7.95503521 5.02559185 5.93345499]\n",
      "Loss : 0.035994422435760495  Real : [8. 8. 2.]  Pred : [7.88409281 7.9632597  1.96855164]\n",
      "Loss : 0.026338771184285483  Real : [7. 3. 3.]  Pred : [7.01884842 3.07716846 3.03752804]\n",
      "Loss : 0.03356213808059692  Real : [1. 8. 7.]  Pred : [0.97168368 7.94071484 7.00018501]\n",
      "Loss : 0.023902437686920165  Real : [6. 2. 1.]  Pred : [6.04555655 2.00346613 0.98846763]\n",
      "Loss : 0.029774062633514405  Real : [4. 1. 8.]  Pred : [4.0044899  0.99838763 7.92190313]\n",
      "Loss : 0.03278866608937581  Real : [6. 2. 3.]  Pred : [6.03597069 2.00443983 3.00618458]\n",
      "Loss : 0.032853612899780275  Real : [4. 4. 5.]  Pred : [3.99351239 4.06228828 4.936584  ]\n",
      "Loss : 0.03681263367335002  Real : [6. 2. 8.]  Pred : [5.98381281 2.06124735 7.96920776]\n",
      "Loss : 0.049560832977294925  Real : [1. 8. 3.]  Pred : [1.00613821 8.07129765 2.99564195]\n",
      "Loss : 0.036265769799550374  Real : [4. 8. 3.]  Pred : [3.96400094 7.95691967 2.98875594]\n",
      "Loss : 0.04135354439417521  Real : [8. 6. 2.]  Pred : [7.95339966 6.04276037 1.99039578]\n",
      "Loss : 0.04328289031982422  Real : [8. 2. 7.]  Pred : [8.01150703 1.9939841  7.05014038]\n",
      "Loss : 0.020180781682332356  Real : [2. 8. 5.]  Pred : [2.00297761 7.97448826 4.98721743]\n",
      "Loss : 0.03892067035039266  Real : [8. 1. 5.]  Pred : [7.98698187 1.00930214 4.89785671]\n",
      "Loss : 0.02143666426340739  Real : [7. 6. 5.]  Pred : [7.01240253 6.00616789 5.02506542]\n",
      "Loss : 0.02560728708902995  Real : [5. 8. 4.]  Pred : [4.9804101  7.99958038 4.0700798 ]\n",
      "Loss : 0.04291402419408163  Real : [6. 8. 6.]  Pred : [5.94925976 7.96335077 5.95540524]\n",
      "Loss : 0.018626614411671957  Real : [7. 4. 1.]  Pred : [7.00783682 4.01408148 1.01057291]\n",
      "Loss : 0.05956361452738444  Real : [3. 7. 1.]  Pred : [2.9635911  6.8880868  1.03049266]\n",
      "Loss : 0.026917288303375243  Real : [5. 2. 7.]  Pred : [4.97293568 2.00156617 6.97802019]\n",
      "Loss : 0.05231549263000488  Real : [1. 7. 4.]  Pred : [0.93746459 6.98345327 3.94968176]\n",
      "Loss : 0.02769849141438802  Real : [4. 7. 3.]  Pred : [3.96873951 6.96318388 3.02809429]\n",
      "Loss : 0.026970953941345216  Real : [1. 5. 6.]  Pred : [1.04364312 4.98720026 6.02295017]\n",
      "Loss : 0.043760047753651936  Real : [8. 2. 5.]  Pred : [7.9527092  1.92812347 5.02169609]\n",
      "Loss : 0.04922562122344971  Real : [8. 3. 6.]  Pred : [8.01449966 2.92970991 5.96048641]\n",
      "Loss : 0.0597087287902832  Real : [7. 2. 2.]  Pred : [7.01106596 2.02879429 1.92849779]\n",
      "Loss : 0.030910346508026123  Real : [2. 2. 8.]  Pred : [2.0412333  2.00768614 7.97944212]\n",
      "Loss : 0.037334295908610024  Real : [7. 5. 7.]  Pred : [7.03082037 4.95858717 6.9429965 ]\n",
      "Loss : 0.027986191908518473  Real : [7. 8. 7.]  Pred : [7.0557375  7.95952225 7.05024481]\n",
      "Loss : 0.03550204197565714  Real : [7. 3. 7.]  Pred : [6.97419834 2.93585467 7.008214  ]\n",
      "Loss : 0.034241480032602946  Real : [3. 1. 5.]  Pred : [3.05277514 0.9894014  4.95094013]\n",
      "Loss : 0.0274130376180013  Real : [6. 2. 4.]  Pred : [6.0404439  1.9949801  3.99449944]\n",
      "Loss : 0.04406827131907145  Real : [7. 6. 1.]  Pred : [6.93760157 5.96519804 0.96758008]\n",
      "Loss : 0.01970771312713623  Real : [7. 2. 7.]  Pred : [7.00081682 1.9763664  7.02421236]\n",
      "Loss : 0.0441874353090922  Real : [6. 5. 4.]  Pred : [5.95814133 4.93897724 4.00170279]\n",
      "Loss : 0.03744878133138021  Real : [1. 6. 1.]  Pred : [0.98699725 6.06248951 1.03374803]\n",
      "Loss : 0.034187790552775064  Real : [8. 8. 7.]  Pred : [7.99474812 7.90923786 7.02497721]\n",
      "Loss : 0.02434906244277954  Real : [7. 1. 2.]  Pred : [7.00065851 0.96217704 1.94127274]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 3\n",
    "batch_size = 25    \n",
    "input_size =1\n",
    "\n",
    "hidden_size = 10\n",
    "enc_model = EncoderRNN(input_size,hidden_size)\n",
    "dec_model = DecoderRNN(input_size,hidden_size,hidden_size)\n",
    "\n",
    "enc_optimizer = torch.optim.Adam(enc_model.parameters(), lr=0.001)\n",
    "dec_optimizer = torch.optim.Adam(dec_model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(10000):\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    d, s = next(random_sequences(max_seq_length, max_seq_length,\n",
    "                     1, 9, \n",
    "                     batch_size))\n",
    "\n",
    "    enc_out = enc_model(torch.from_numpy(d.T),torch.from_numpy(s))\n",
    "    \n",
    "    all_out = torch.from_numpy(np.zeros([max_seq_length,batch_size,input_size]))\n",
    "    #print(all_out.shape)\n",
    "    for j in range(max_seq_length):\n",
    "\n",
    "        dec_out = dec_model(enc_out)\n",
    "        all_out[j] = dec_out\n",
    "    \n",
    "    d = torch.from_numpy(d).unsqueeze(-1).transpose(0,1)\n",
    "    \n",
    "    loss = torch.nn.L1Loss()(all_out,d)\n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    enc_optimizer.step()\n",
    "    dec_optimizer.step()\n",
    "    enc_model.hidden = None\n",
    "    dec_model.hidden = None\n",
    "    #d = d.squeeze(-1).detach().numpy().T[0]\n",
    "    #all_out = all_out.squeeze(-1).detach().numpy().T[0]\n",
    "    \n",
    "    d = d[:,0].squeeze(-1).detach().numpy()\n",
    "    \n",
    "    all_out = all_out[:,0].squeeze(-1).detach().numpy()\n",
    "    if i % 100 == 0:\n",
    "        print(\"Loss : {}  Real : {}  Pred : {}\".format(loss.detach().numpy(),d,all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
