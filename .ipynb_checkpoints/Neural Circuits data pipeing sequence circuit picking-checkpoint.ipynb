{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "batch_size = 15\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.ToTensor()),\n",
    "        batch_size = 10000, shuffle=True)\n",
    "\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if device != 'cpu':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "num_classes=10\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=3, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        self.hidden = None\n",
    "        #self.init_weights()\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths):\n",
    "       # input_seqs = input_seqs.type(torch.FloatTensor)\n",
    "        #packed = torch.nn.utils.rnn.pack_padded_sequence(input_seqs, input_lengths)\n",
    "        \n",
    "        outputs, self.hidden = self.gru(input_seqs, self.hidden)\n",
    "        #outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs \n",
    "        return outputs\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.gru.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal(param)\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,output_size,hidden_size,enc_hidden_size,n_layers=3):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(enc_hidden_size, hidden_size, n_layers, bidirectional=True)\n",
    "        self.hidden = None\n",
    "       # self.init_weights()\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, encoder_outputs):\n",
    "        rnn_in = encoder_outputs[-1].unsqueeze(0)\n",
    "           \n",
    "        outputs, self.hidden = self.gru(rnn_in, self.hidden)\n",
    "        outputs = self.concat(outputs)\n",
    "        out = self.out(outputs)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.gru.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CircuitNet(nn.Module):\n",
    "    def __init__(self,num_classes=12,num_lstm_layers=3):\n",
    "        super(CircuitNet, self).__init__()\n",
    "        \n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.circuits_to_use = 3\n",
    "        self.max_circuits = 6\n",
    "        self.n_circuits = 10\n",
    "        self.circuit_in_dim = 4\n",
    "        \n",
    "        self.circuit_out_shape = 16*3\n",
    "        self.init_layer =  nn.Linear(28*28, self.circuit_out_shape*5)\n",
    "        \n",
    "        #circuit picker able to send instructions to data pipe\n",
    "        self.hidden_data_num = 10\n",
    "        \n",
    "        hidden_size = 3\n",
    "        self.encoder = EncoderRNN(self.circuit_out_shape,hidden_size,n_layers = 3)\n",
    "        self.decoder = DecoderRNN(self.n_circuits+self.hidden_data_num,hidden_size,hidden_size,n_layers = 3)\n",
    "        \n",
    "        self.circuit_picker = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 3, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.MaxPool1d(2, stride=2),  \n",
    "            nn.Conv1d(16, 8, 3, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.circuit_picker_out = nn.LSTM(1568,(self.n_circuits *  self.circuits_to_use), self.num_lstm_layers)\n",
    "        self.hidden = None\n",
    "        \n",
    "        #data pipe\n",
    "        self.data_pipe = nn.Sequential(\n",
    "            nn.Conv1d(1, 4, 4, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.Conv1d(4, 8, 4, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(2, stride=2), \n",
    "            \n",
    "            nn.Conv1d(8, 4, 4, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.Conv1d(4, 8, 4, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(2, stride=2)\n",
    "        )\n",
    "        \n",
    "        #self.data_pipe_out = nn.Linear(245, (self.circuits_to_use * self.circuit_in_dim))\n",
    "        self.data_pipe_out = nn.Linear(298, self.circuit_in_dim)\n",
    "        \n",
    "        #define circuits\n",
    "        self.circuits = []\n",
    "        for i in range(self.n_circuits):\n",
    "            self.circuits.append(nn.Sequential(\n",
    "                nn.Conv1d(1, 8, kernel_size=2, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(8),\n",
    "                nn.ReLU(True),\n",
    "                #nn.MaxPool2d(kernel_size=2, stride=2)))\n",
    "            \n",
    "                nn.Conv1d(8, 16, kernel_size=2, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.ReLU(True),\n",
    "                \n",
    "                nn.MaxPool1d(kernel_size=2, stride=2)))\n",
    "        \n",
    "        #connect circuits (this is temporary)\n",
    "        self.connect = nn.Sequential(\n",
    "            nn.Linear(1440, 28*28))\n",
    "        \n",
    "        #final circuit to prediction\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(144, num_classes))\n",
    "        \n",
    "    #connection input shape [batch_size,num_circuits,circuit_out_shape]\n",
    "    def connection(self,x,verbose,circuit_dropout,circuit_ban_rate):\n",
    "        \n",
    "        x_pad = F.pad(x.transpose(1,2),(0,self.max_circuits-x.size(1))).transpose(1,2)\n",
    "        x_flat = x_pad.contiguous().view(x_pad.size(0),1,-1)\n",
    "        \n",
    "        ###pick circuits##################\n",
    "        #circuit_pick = self.circuit_picker(x_flat)\n",
    "        #circuit_pick = circuit_pick.view(circuit_pick.size(0),1, -1)\n",
    "        \n",
    "        #circuit_pick, self.hidden = self.circuit_picker_out(circuit_pick,self.hidden)\n",
    "        \n",
    "        \n",
    "        #circuit_pick = circuit_pick.view(circuit_pick.size(0),self.circuits_to_use,self.n_circuits)\n",
    "        #circuit_pick = circuit_pick.max(2)[1]\n",
    "        \n",
    "        enc_in = x.transpose(0,1)\n",
    "        enc_out = self.encoder(enc_in,[5]*batch_size)\n",
    "        all_dec_out = []\n",
    "        hidden_instructions = []\n",
    "        for i in range(self.circuits_to_use):\n",
    "            dec_out = self.decoder(enc_out)\n",
    "            all_dec_out.append(dec_out[:,:,0:self.n_circuits])\n",
    "            hidden_instructions.append(dec_out[:,:,self.n_circuits:])\n",
    "            \n",
    "        all_dec_out = torch.cat(all_dec_out,0).transpose(0,1)\n",
    "        hidden_instructions = torch.cat(hidden_instructions,0).transpose(0,1)\n",
    "\n",
    "        circuit_pick = all_dec_out\n",
    "        circuit_pick = circuit_pick.max(2)[1]\n",
    "        data_pipe_in = torch.cat((circuit_pick.unsqueeze(1).type(torch.cuda.FloatTensor),x_flat),2)\n",
    "        \n",
    "        #print(all_dec_out.shape)\n",
    "        ###################################\n",
    "\n",
    "        #data_pipe = self.data_pipe(data_pipe_in)\n",
    "        #data_pipe = data_pipe.view(data_pipe.size(0), -1)\n",
    "        #data_pipe = data_pipe_in.squeeze(1)\n",
    "        #data_pipe = self.data_pipe_out(data_pipe)\n",
    "        #data_pipe = data_pipe.view(data_pipe.size(0),self.circuits_to_use,self.circuit_in_dim)\n",
    "        bans = np.random.randint(self.n_circuits,size=int(circuit_ban_rate*self.n_circuits))\n",
    "        circuit_out = []\n",
    "        for i in range(self.circuits_to_use):\n",
    "            circuit_idx = circuit_pick[:,i]\n",
    "            if(verbose):\n",
    "                print(str(circuit_idx.cpu().numpy()))\n",
    "                \n",
    "            batch = []\n",
    "            for b in range(batch_size):\n",
    "                \n",
    "                hid_instruct = hidden_instructions[b,i]\n",
    "                #data = data_pipe[b][i]\n",
    "                data_in = torch.cat((x_flat[b],hidden_instructions[b,i].unsqueeze(0)),1)\n",
    "                data_pipe = self.data_pipe_out(data_in)\n",
    "                #data = data.unsqueeze(0).unsqueeze(0)\n",
    "                circuit = self.circuits[circuit_idx[b]](data_pipe.unsqueeze(0))\n",
    "                #apply dropout\n",
    "                if randint(1,100) < circuit_dropout*100 or circuit_idx[b] in bans:\n",
    "                    circuit = torch.zeros(1,16,3,device=device)  \n",
    "                \n",
    "                batch.append(circuit)\n",
    "            batch = torch.cat(batch,0)\n",
    "            circuit_out.append(batch)\n",
    "        if(verbose):\n",
    "            print(\" \")\n",
    "        circuit_out = torch.cat(circuit_out,1).transpose(1,2)\n",
    "        return circuit_out\n",
    "    \n",
    "    def forward(self, x, verbose,circuit_dropout=0.5,circuit_ban_rate = 0.5):\n",
    "        #print(self.encoder.gru.named_parameters())\n",
    "        x_flat = x.view(x.size(0),-1)\n",
    "        init = self.init_layer(x_flat)\n",
    "        \n",
    "        init = init.view(init.size(0),5,init.size(1)/5)\n",
    "        circuit_out1 = self.connection(init,verbose,circuit_dropout,circuit_ban_rate)\n",
    "        #circuit_out_flat1 = circuit_out1.contiguous().view(circuit_out1.size(0), -1)\n",
    "        self.encoder.hidden = None\n",
    "        self.decoder.hidden = None\n",
    "        \n",
    "        circuit_out2 = self.connection(circuit_out1,verbose,circuit_dropout,circuit_ban_rate)\n",
    "        circuit_out_flat2 = circuit_out2.contiguous().view(circuit_out2.size(0), -1)\n",
    "        #print(circuit_out1.shape)\n",
    "        #connect = self.connect(circuit_out_flat1)\n",
    "        #connect = connect.view(connect.size(0),1,28,28)\n",
    "        \n",
    "       # circuit_out2 = self.connection(connect,verbose)\n",
    "        #circuit_out_flat2 = circuit_out2.view(circuit_out2.size(0), -1)\n",
    "        out = self.output(circuit_out_flat2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        \n",
    "\n",
    "if device != 'cpu':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "model = CircuitNet().to(device)\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device)[0:batch_size], target.to(device)[0:batch_size]\n",
    "    output = model(data,False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 0  Loss : 2.53479  Real : [4 5 0 4]  Pred : [ 6 10  1  6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 50  Loss : 1.80847  Real : [1 3 5 0]  Pred : [1 4 4 4]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 100  Loss : 2.07389  Real : [4 2 4 5]  Pred : [1 1 1 1]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 150  Loss : 1.98158  Real : [1 3 3 5]  Pred : [1 1 1 1]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 200  Loss : 1.94690  Real : [5 3 0 3]  Pred : [1 1 1 1]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 250  Loss : 1.81076  Real : [5 4 2 1]  Pred : [0 3 3 3]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 300  Loss : 2.07515  Real : [1 2 2 0]  Pred : [1 4 4 1]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 350  Loss : 1.75225  Real : [2 1 1 3]  Pred : [0 3 1 1]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 400  Loss : 1.86998  Real : [2 2 3 0]  Pred : [1 1 1 1]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 450  Loss : 1.87496  Real : [0 4 0 1]  Pred : [1 2 2 2]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 500  Loss : 1.91482  Real : [3 5 1 4]  Pred : [1 1 1 1]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " \n",
      "Index : 550  Loss : 1.72107  Real : [4 4 3 1]  Pred : [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer2 = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "iter_print = 50\n",
    "\n",
    "for batch_idx, (data_, target_) in enumerate(train_loader):\n",
    "    #print(target)\n",
    "   \n",
    "    \n",
    "    \n",
    "    #train on digits 1-6, then train on 7-9 to check adaptability\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            \n",
    "            rand = randint(0, 3000)\n",
    "            indices = np.where(target_<6)[0][rand:rand+batch_size]\n",
    "            #indices = \n",
    "            data = np.take(data_,indices,axis = 0)\n",
    "            target = np.take(target_,indices)\n",
    "\n",
    "           # print(np.where(target_<6)[0][0:batch_size])\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            verbose = False\n",
    "            if(i % iter_print == 0):\n",
    "                verbose = True\n",
    "            output = model(data,verbose,circuit_dropout=0.7,circuit_ban_rate = 0.5)\n",
    "            loss =  nn.CrossEntropyLoss()(output, target)\n",
    "            #loss = (output-target)**2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.hidden = None\n",
    "            model.encoder.hidden = None\n",
    "            model.decoder.hidden = None\n",
    "            if(i % iter_print == 0):\n",
    "                print(\"Index : {}  Loss : {:.5f}  Real : {}  Pred : {}\".format(i,\n",
    "                                                                               loss[0],target[0:4].cpu().numpy(),\n",
    "                                                                               output[0:4].max(1)[1].cpu().numpy()))\n",
    "        except:\n",
    "            continue\n",
    "    print(\"switch\")\n",
    "    for i in range(500):\n",
    "        try:\n",
    "            rand = randint(0, 3000)\n",
    "            indices = np.where(target_>6)[0][rand:rand+batch_size]\n",
    "            #indices = \n",
    "            data = np.take(data_,indices,axis = 0)\n",
    "            target = np.take(target_,indices)\n",
    "\n",
    "           # print(np.where(target_<6)[0][0:batch_size])\n",
    "            datcircuit_out_flat2a, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer2.zero_grad()\n",
    "            verbose = False\n",
    "            if(batch_idx % iter_print == 0):\n",
    "                verbose = False\n",
    "            output = model(data,verbose)\n",
    "            loss =  nn.CrossEntropyLoss()(output, target)\n",
    "            #loss = (output-target)**2\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "\n",
    "            model.hidden = None\n",
    "            model.encoder.hidden = None\n",
    "            model.decoder.hidden = None\n",
    "            if(i % 1 == 0):\n",
    "                print(\"Index : {}  Loss : {:.5f}  Real : {}  Pred : {}\".format(i,\n",
    "                                                                               loss[0],target[0:4].cpu().numpy(),\n",
    "                                                                               output[0:4].max(1)[1].cpu().numpy()))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fsdg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
