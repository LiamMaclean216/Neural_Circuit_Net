{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "batch_size = 15\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.ToTensor()),\n",
    "        batch_size = 10000, shuffle=True)\n",
    "\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if device != 'cpu':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "num_classes=10\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=3, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        self.hidden = None\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths):\n",
    "       # input_seqs = input_seqs.type(torch.FloatTensor)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seqs, input_lengths)\n",
    "        outputs, self.hidden = self.gru(packed, self.hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs \n",
    "        return outputs\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,output_size,hidden_size,enc_hidden_size,n_layers=3):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(enc_hidden_size, hidden_size, n_layers, bidirectional=True)\n",
    "        self.hidden = None\n",
    "       \n",
    "        self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, encoder_outputs):\n",
    "        rnn_in = encoder_outputs[-1].unsqueeze(0)\n",
    "           \n",
    "        outputs, self.hidden = self.gru(rnn_in, self.hidden)\n",
    "        outputs = self.concat(outputs)\n",
    "        out = self.out(outputs)\n",
    "        \n",
    "        return F.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [15 x 144], m2: [240 x 12] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-64243068fe08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-64243068fe08>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, verbose, circuit_dropout, circuit_ban_rate)\u001b[0m\n\u001b[1;32m    156\u001b[0m        \u001b[0;31m# circuit_out2 = self.connection(connect,verbose)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m#circuit_out_flat2 = circuit_out2.view(circuit_out2.size(0), -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuit_out_flat1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [15 x 144], m2: [240 x 12] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249"
     ]
    }
   ],
   "source": [
    "class CircuitNet(nn.Module):\n",
    "    def __init__(self,num_classes=12,num_lstm_layers=3):\n",
    "        super(CircuitNet, self).__init__()\n",
    "        \n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.circuits_to_use = 3\n",
    "        self.n_circuits = 10\n",
    "        self.circuit_in_dim = 4\n",
    "        \n",
    "        self.circuit_out_shape = 16*3\n",
    "        self.init_layer =  nn.Linear(28*28, self.circuit_out_shape*5)\n",
    "        \n",
    "        #circuit picker able to send instructions to data pipe\n",
    "        self.hidden_data_num = 10\n",
    "        \n",
    "        hidden_size = 3\n",
    "        self.encoder = EncoderRNN(self.circuit_out_shape,hidden_size,n_layers = 3)\n",
    "        self.decoder = DecoderRNN(self.n_circuits+self.hidden_data_num,hidden_size,hidden_size,n_layers = 3)\n",
    "        \n",
    "        self.circuit_picker = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 3, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.MaxPool1d(2, stride=2),  \n",
    "            nn.Conv1d(16, 8, 3, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.circuit_picker_out = nn.LSTM(1568,(self.n_circuits *  self.circuits_to_use), self.num_lstm_layers)\n",
    "        self.hidden = None\n",
    "        \n",
    "        #data pipe\n",
    "        self.data_pipe = nn.Sequential(\n",
    "            nn.Conv1d(1, 4, 4, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.Conv1d(4, 8, 4, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(2, stride=2), \n",
    "            \n",
    "            nn.Conv1d(8, 4, 4, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.Conv1d(4, 8, 4, stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(2, stride=2)\n",
    "        )\n",
    "        \n",
    "        #self.data_pipe_out = nn.Linear(245, (self.circuits_to_use * self.circuit_in_dim))\n",
    "        self.data_pipe_out = nn.Linear(250, self.circuit_in_dim)\n",
    "        \n",
    "        #define circuits\n",
    "        self.circuits = []\n",
    "        for i in range(self.n_circuits):\n",
    "            self.circuits.append(nn.Sequential(\n",
    "                nn.Conv1d(1, 8, kernel_size=2, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(8),\n",
    "                nn.ReLU(True),\n",
    "                #nn.MaxPool2d(kernel_size=2, stride=2)))\n",
    "            \n",
    "                nn.Conv1d(8, 16, kernel_size=2, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.ReLU(True),\n",
    "                \n",
    "                nn.MaxPool1d(kernel_size=2, stride=2)))\n",
    "        \n",
    "        #connect circuits (this is temporary)\n",
    "        self.connect = nn.Sequential(\n",
    "            nn.Linear(1440, 28*28))\n",
    "        \n",
    "        #final circuit to prediction\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(144, num_classes))\n",
    "        \n",
    "    #connection input shape [batch_size,num_circuits,circuit_out_shape]\n",
    "    def connection(self,x,verbose,circuit_dropout,circuit_ban_rate):\n",
    "        x_flat = x.view(x.size(0),1,-1)\n",
    "        ###pick circuits##################\n",
    "        #circuit_pick = self.circuit_picker(x_flat)\n",
    "        #circuit_pick = circuit_pick.view(circuit_pick.size(0),1, -1)\n",
    "        \n",
    "        #circuit_pick, self.hidden = self.circuit_picker_out(circuit_pick,self.hidden)\n",
    "        \n",
    "        \n",
    "        #circuit_pick = circuit_pick.view(circuit_pick.size(0),self.circuits_to_use,self.n_circuits)\n",
    "        #circuit_pick = circuit_pick.max(2)[1]\n",
    "        \n",
    "        enc_in = x.transpose(0,1)\n",
    "        enc_out = self.encoder(enc_in,[5]*batch_size)\n",
    "        all_dec_out = []\n",
    "        hidden_instructions = []\n",
    "        for i in range(self.circuits_to_use):\n",
    "            dec_out = self.decoder(enc_out)\n",
    "            all_dec_out.append(dec_out[:,:,0:self.n_circuits])\n",
    "            hidden_instructions.append(dec_out[:,:,self.n_circuits:])\n",
    "            \n",
    "        all_dec_out = torch.cat(all_dec_out,0).transpose(0,1)\n",
    "        hidden_instructions = torch.cat(hidden_instructions,0).transpose(0,1)\n",
    "\n",
    "        circuit_pick = all_dec_out\n",
    "        circuit_pick = circuit_pick.max(2)[1]\n",
    "        data_pipe_in = torch.cat((circuit_pick.unsqueeze(1).type(torch.cuda.FloatTensor),x_flat),2)\n",
    "        \n",
    "        #print(all_dec_out.shape)\n",
    "        ###################################\n",
    "\n",
    "        #data_pipe = self.data_pipe(data_pipe_in)\n",
    "        #data_pipe = data_pipe.view(data_pipe.size(0), -1)\n",
    "        #data_pipe = data_pipe_in.squeeze(1)\n",
    "        #data_pipe = self.data_pipe_out(data_pipe)\n",
    "        #data_pipe = data_pipe.view(data_pipe.size(0),self.circuits_to_use,self.circuit_in_dim)\n",
    "        bans = np.random.randint(self.n_circuits,size=int(circuit_ban_rate*self.n_circuits))\n",
    "        circuit_out = []\n",
    "        for i in range(self.circuits_to_use):\n",
    "            circuit_idx = circuit_pick[:,i]\n",
    "            if(verbose):\n",
    "                print(str(circuit_idx.cpu().numpy()))\n",
    "                \n",
    "            batch = []\n",
    "            for b in range(batch_size):\n",
    "                \n",
    "                hid_instruct = hidden_instructions[b,i]\n",
    "                #data = data_pipe[b][i]\n",
    "                data_in = torch.cat((x_flat[b],hidden_instructions[b,i].unsqueeze(0)),1)\n",
    "                data_pipe = self.data_pipe_out(data_in)\n",
    "                #data = data.unsqueeze(0).unsqueeze(0)\n",
    "                circuit = self.circuits[circuit_idx[b]](data_pipe.unsqueeze(0))\n",
    "                #apply dropout\n",
    "                if randint(1,100) < circuit_dropout*100 or circuit_idx[b] in bans:\n",
    "                    circuit = torch.zeros(1,16,3,device=device)  \n",
    "                \n",
    "                batch.append(circuit)\n",
    "            batch = torch.cat(batch,0)\n",
    "            circuit_out.append(batch)\n",
    "        \n",
    "        circuit_out = torch.cat(circuit_out,1)\n",
    "        \n",
    "        return circuit_out\n",
    "    \n",
    "    def forward(self, x, verbose,circuit_dropout=0.5,circuit_ban_rate = 0.5):\n",
    "        x_flat = x.view(x.size(0),-1)\n",
    "        init = self.init_layer(x_flat)\n",
    "        \n",
    "        init = init.view(init.size(0),5,init.size(1)/5)\n",
    "        circuit_out1 = self.connection(init,verbose,circuit_dropout,circuit_ban_rate)\n",
    "        circuit_out_flat1 = circuit_out1.view(circuit_out1.size(0), -1)\n",
    "        \n",
    "        #connect = self.connect(circuit_out_flat1)\n",
    "        #connect = connect.view(connect.size(0),1,28,28)\n",
    "        \n",
    "       # circuit_out2 = self.connection(connect,verbose)\n",
    "        #circuit_out_flat2 = circuit_out2.view(circuit_out2.size(0), -1)\n",
    "        out = self.output(circuit_out_flat1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        \n",
    "\n",
    "if device != 'cpu':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "model = CircuitNet().to(device)\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device)[0:batch_size], target.to(device)[0:batch_size]\n",
    "    output = model(data,False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer2 = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "iter_print = 50\n",
    "\n",
    "for batch_idx, (data_, target_) in enumerate(train_loader):\n",
    "    #print(target)\n",
    "   \n",
    "    \n",
    "    \n",
    "    #train on digits 1-6, then train on 7-9 to check adaptability\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            rand = randint(0, 3000)\n",
    "            indices = np.where(target_<6)[0][rand:rand+batch_size]\n",
    "            #indices = \n",
    "            data = np.take(data_,indices,axis = 0)\n",
    "            target = np.take(target_,indices)\n",
    "\n",
    "           # print(np.where(target_<6)[0][0:batch_size])\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            verbose = False\n",
    "            if(i % iter_print == 0):\n",
    "                verbose = True\n",
    "            output = model(data,verbose,circuit_dropout=0.7,circuit_ban_rate = 0.5)\n",
    "            loss =  nn.CrossEntropyLoss()(output, target)\n",
    "            #loss = (output-target)**2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.hidden = None\n",
    "            model.encoder.hidden = None\n",
    "            model.decoder.hidden = None\n",
    "            if(i % iter_print == 0):\n",
    "                print(\"Index : {}  Loss : {:.5f}  Real : {}  Pred : {}\".format(i,\n",
    "                                                                               loss[0],target[0:4].cpu().numpy(),\n",
    "                                                                               output[0:4].max(1)[1].cpu().numpy()))\n",
    "        except:\n",
    "            continue\n",
    "    print(\"switch\")\n",
    "    for i in range(500):\n",
    "        try:\n",
    "            rand = randint(0, 3000)\n",
    "            indices = np.where(target_>6)[0][rand:rand+batch_size]\n",
    "            #indices = \n",
    "            data = np.take(data_,indices,axis = 0)\n",
    "            target = np.take(target_,indices)\n",
    "\n",
    "           # print(np.where(target_<6)[0][0:batch_size])\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer2.zero_grad()\n",
    "            verbose = False\n",
    "            if(batch_idx % iter_print == 0):\n",
    "                verbose = False\n",
    "            output = model(data,verbose)\n",
    "            loss =  nn.CrossEntropyLoss()(output, target)\n",
    "            #loss = (output-target)**2\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "\n",
    "            model.hidden = None\n",
    "            model.encoder.hidden = None\n",
    "            model.decoder.hidden = None\n",
    "            if(i % 1 == 0):\n",
    "                print(\"Index : {}  Loss : {:.5f}  Real : {}  Pred : {}\".format(i,\n",
    "                                                                               loss[0],target[0:4].cpu().numpy(),\n",
    "                                                                               output[0:4].max(1)[1].cpu().numpy()))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fsdg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
